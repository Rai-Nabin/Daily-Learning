# RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback
- Research Paper Link: https://arxiv.org/pdf/2309.00267.pdf
- Year of Release: 2023
- Published By: Google Research
## Abstract
**Reinforcement Learning from Human Feedback (RLHF):**

- This is a technique used to train large language models (LLMs). LLMs are AI systems that can generate human-like text based on the input they receive.
- RLHF involves using feedback from humans to guide the learning process of these models so that their outputs align with human preferences.

**Challenges with RLHF:**

- The main challenge here is gathering high-quality feedback from humans which can be time-consuming and expensive because it requires active participation and expertise.

**Reinforcement Learning from AI Feedback (RLAIF):**

- RLAIF is an alternative approach where instead of relying on humans for preference labels, an existing LLM generates them itself by predicting what kind of responses would be preferred in different situations.

**Performance comparison between RLAIF and RLHF:**

- In tasks such as summarization or dialogue generation, RLAIF has shown comparable or even superior performance compared to traditional methods like RLHF according to evaluations done by human raters.

**Outperforming supervised fine-tuned baseline:**

- A "supervised fine-tuned baseline" refers to a model trained under supervision i.e., provided with correct answers during training phase then finely adjusted ("fine-tuned") for specific tasks later on.
- Even when the size of the model generating preference labels was equal led out against policy size in terms of parameters - meaning no extra computational advantage was given - RLAIF still managed better results than this standard setup indicating its efficiency.

**Direct prompting vs distillation into reward model:**

- Directly asking ('prompting') LLMs about reward scores achieved better results than first converting ('distilling') those same predictions into a separate 'reward' system within canonical/traditional setups for applying reinforcement learning via AI feedback.

**Potential of RLAIF:**

- The results suggest that RLAIF has the potential to achieve performance at par with humans while also addressing scalability issues associated with RLHF.
# Introduction
**Reinforcement Learning from  Human Feedback (RLHF):**

- RLHF is a technique used to train language models like ChatGPT and Bard.
- The goal of RLHF is to align these language models with human preferences, meaning it aims for the model's output or behavior to match what a human would find useful or desirable.

**Role of Reinforcement Learning:**

- Reinforcement Learning (RL) is a type of machine learning where an agent learns how to behave in an environment by performing actions and receiving rewards or penalties in return.
- In this context, the 'agent' could be considered as our language model which needs training. The 'environment' can be thought as all possible inputs and outputs for our model.

**Optimization on Complex Objectives:**

- Traditional supervised fine-tuning methods require clearly  defined labels for every instance in their training dataset; however, many real-world problems don't have such neat solutions available upfront. 
- This makes reinforcement learning particularly suitable when dealing with complex sequence-level objectives like those found when training large-scale conversational agents.

**Challenge in scaling RLHF:**

- The main challenge of scaling RLHF is its reliance on high-quality human preference labels.
- This dependence can make RLHF time-consuming and expensive to implement at a large scale.

**Artificially Generated Labels:**

- The potential of using labels generated by AI, specifically large language models (LLMs), as a substitute for human-generated labels in reinforcement learning.
- Large Language Models (LLMs) show high alignment with human judgment, making them suitable for generating these artificial labels.

**Reinforcement Learning from AI Feedback (RLAIF):**

- It involves using feedback from an AI, such as an LLM, instead of human feedback to guide the learning process of another AI system.

![RLAIF](./images/rlaif.png)

**Study Overview:**

- The research investigates the impact of two reinforcement learning methods, RLAIF and RLHF, on three text generation tasks:
    - Summarization
    - Helpful dialogue generation
    - Harmless dialogue generation
- Both RLAIF and RLHF are preferred by humans over the Supervised Fine-Tuning (SFT) baseline.
- There is no significant difference between the preference for these two methods.
- For harmless dialogue generation task, RLAIF scored higher than RLHF.
- These results suggest that AI-generated feedback (RLAIF) could be a viable alternative to human feedback (RLHF), as it doesn't rely on human annotation while offering scalability.

![RLAIF vs RLHF Win Rates](./images/rlaif-win-rates.png)

**The main contributions of this research:**

1. Performance Comparison: 
    - RLAIF performs comparably or better than RLHF in tasks such as summarization, and generating helpful and harmless dialogues.
2. Improvement over SFT Policy:  
    - RLAIF can improve upon a Supervised Fine-Tuning (SFT) policy even when the Language Model (LLM) labeler is of equal size to the policy.
3. Direct Prompting Advantage:
    - Directly prompting LLM for reward scores during Reinforcement Learning can outperform traditional setup where a reward model is trained on LLM preferences.
4. Techniques for AI Labels Generation:
    - The study compares different techniques for generating AI labels and identifies optimal settings for practitioners using RLAIF.

# Methodology
