# RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback
- Research Paper Link: https://arxiv.org/pdf/2309.00267.pdf
- Year of Release: 2023
- Published By: Google Research
## Abstract
**Reinforcement Learning from Human Feedback (RLHF):**

- This is a technique used to train large language models (LLMs). LLMs are AI systems that can generate human-like text based on the input they receive.
- RLHF involves using feedback from humans to guide the learning process of these models so that their outputs align with human preferences.

**Challenges with RLHF:**

- The main challenge here is gathering high-quality feedback from humans which can be time-consuming and expensive because it requires active participation and expertise.

**Reinforcement Learning from AI Feedback (RLAIF):**

- RLAIF is an alternative approach where instead of relying on humans for preference labels, an existing LLM generates them itself by predicting what kind of responses would be preferred in different situations.

**Performance comparison between RLAIF and RLHF:**

- In tasks such as summarization or dialogue generation, RLAIF has shown comparable or even superior performance compared to traditional methods like RLHF according to evaluations done by human raters.

**Outperforming supervised fine-tuned baseline:**

- A "supervised fine-tuned baseline" refers to a model trained under supervision i.e., provided with correct answers during training phase then finely adjusted ("fine-tuned") for specific tasks later on.
- Even when the size of the model generating preference labels was equal led out against policy size in terms of parameters - meaning no extra computational advantage was given - RLAIF still managed better results than this standard setup indicating its efficiency.

**Direct prompting vs distillation into reward model:**

- Directly asking ('prompting') LLMs about reward scores achieved better results than first converting ('distilling') those same predictions into a separate 'reward' system within canonical/traditional setups for applying reinforcement learning via AI feedback.

**Potential of RLAIF:**

- The results suggest that RLAIF has the potential to achieve performance at par with humans while also addressing scalability issues associated with RLHF.
# Introduction
**Reinforcement Learning from  Human Feedback (RLHF):**

- RLHF is a technique used to train language models like ChatGPT and Bard.
- The goal of RLHF is to align these language models with human preferences, meaning it aims for the model's output or behavior to match what a human would find useful or desirable.

**Role of Reinforcement Learning:**

- Reinforcement Learning (RL) is a type of machine learning where an agent learns how to behave in an environment by performing actions and receiving rewards or penalties in return.
- In this context, the 'agent' could be considered as our language model which needs training. The 'environment' can be thought as all possible inputs and outputs for our model.

**Optimization on Complex Objectives:**

- Traditional supervised fine-tuning methods require clearly  defined labels for every instance in their training dataset; however, many real-world problems don't have such neat solutions available upfront. 
- This makes reinforcement learning particularly suitable when dealing with complex sequence-level objectives like those found when training large-scale conversational agents.

**Challenge in scaling RLHF:**

- The main challenge of scaling RLHF is its reliance on high-quality human preference labels.
- This dependence can make RLHF time-consuming and expensive to implement at a large scale.

**Artificially Generated Labels:**

- The potential of using labels generated by AI, specifically large language models (LLMs), as a substitute for human-generated labels in reinforcement learning.
- Large Language Models (LLMs) show high alignment with human judgment, making them suitable for generating these artificial labels.

**Reinforcement Learning from AI Feedback (RLAIF):**

- It involves using feedback from an AI, such as an LLM, instead of human feedback to guide the learning process of another AI system.

![RLAIF](./images/rlaif.png)

**Study Overview:**

- The research investigates the impact of two reinforcement learning methods, RLAIF and RLHF, on three text generation tasks:
    - Summarization
    - Helpful dialogue generation
    - Harmless dialogue generation
- Both RLAIF and RLHF are preferred by humans over the Supervised Fine-Tuning (SFT) baseline.
- There is no significant difference between the preference for these two methods.
- For harmless dialogue generation task, RLAIF scored higher than RLHF.
- These results suggest that AI-generated feedback (RLAIF) could be a viable alternative to human feedback (RLHF), as it doesn't rely on human annotation while offering scalability.

![RLAIF vs RLHF Win Rates](./images/rlaif-win-rates.png)

**The main contributions of this research:**

1. Performance Comparison: 
    - RLAIF performs comparably or better than RLHF in tasks such as summarization, and generating helpful and harmless dialogues.
2. Improvement over SFT Policy:  
    - RLAIF can improve upon a Supervised Fine-Tuning (SFT) policy even when the Language Model (LLM) labeler is of equal size to the policy.
3. Direct Prompting Advantage:
    - Directly prompting LLM for reward scores during Reinforcement Learning can outperform traditional setup where a reward model is trained on LLM preferences.
4. Techniques for AI Labels Generation:
    - The study compares different techniques for generating AI labels and identifies optimal settings for practitioners using RLAIF.

# Methodology
**Preferences Labeling with LLMs**

- "Off-the-shelf" LLMs are used to annotate preferences between pairs of candidate responses. These LLMs are pre-trained or instruction-tuned on a large dataset for general language understanding but not fine-tuned for a specific down-stream task.
- Given a piece of text and two potential responses, the LLM determines which response is more preferable.
- The prompt is structured as follows:

![](./images/rlaif-prompt.png)
![](./images/rlaif-prompt-1.png)

**Note:** 

1. Premeable: This is an introduction and instructions describing the task at hand.
2. Few-shot exemplars (optional): These are example input contexts, pairs of responses, optional chain-of-thought rationale, and a preference label.
3. Sample to annotate: This includes an input context and a pair of responses that need to be labeled by the LLM.
4. Ending: Ending text to prompt the LLM (e.g. "Preference Response=")

- Preference Labels Method:
    - An LLM is given a prompt to generate responses based on the provided prompt.
    - Log-probabilities of generating tokens "1" and "2" are extracted from these generated responses.
    - These log-probabilities are converted into a probability distribution using the softmax function.
    - The resulting probability distribution, known as a preference distribution, represents how much preference an LLM has for one response over another.
- Types of Prompts Experimented:
    - Base Prompt - Simply asks which response is better.
    - Detailed Prompt - Provides more specific rating instructions, similar to what human annotators would receive.
- Use of In-context Learning:
    - High-quality examples are chosen across various topics.

![](./images/preference-labels.png)

**Addressing Position Bias**

- This is a phenomenon in machine learning where the order of presenting options or candidates to an LLM  can influence its preference.
- The paper suggests that position bias becomes more pronounced with smaller sizes of LLM labelers. That means smaller models are more likely to be influenced by the order in which they receive information.
- To counteract this bias, each pair of candidates is evaluated twice - once in original order and once with reversed order.
- After both evaluations (original and reversed), results from both instances are averaged out for final preference distribution.

![](./images/position-bias.png)

**Chain-of-thought Reasoning**

- The researchers are experimenting with a method called "chain-of-thought" (CoT) reasoning to improve the inference process of AI models.
- This involves a two-step procedure:
    1. The standard prompt is replaced with a sentence asking for a detailed explanation.
    2. The LLM response is decoded and combined with the original prompt to obtain a preference distribution.

![](./images/generate-ai-preference.png)

- In zero-shot prompts, the LLM isn't given any examples of what reasoning should look like.
![](./images/zero-shot-prompt.png)

- In few-shot prompts, examples of CoT reasoning are provided for the model to follow.

![](./images/few-shot-prompt.png)

**Distilled RLAIF**

- Preference labels are obtained from an LLM.
- A Reward Model (RM) is trained using these preference labels.
- Apply a cross-entropy loss to the softmax of the reward scores generated by the Reward Model (RM).
- Training RM on AI-generated labels is similar to model distillation. This technique involves training a simpler model to mimic the behavior of a larger, more complex one.
- The RLAIF policy model learns over time through a process of trial and error, aiming to generate better responses that will receive higher rewards from the RM.

**Direct RLAIF**

- Directly uses feedback from a Large Language Model (LLM) as the reward signal in Reinforcement Learning (RL).
- Bypasses the need for an intermediate Reward Model (RM) that approximates LLM's preferences.
- The LLM rates the quality of generated output on a scale of 1 to 10 based on criteria like factuality and coherence.
- It calculates the likelihood of each score from 1 to 10. These likelihoods are normalized into a probability distribution.
- A weighted score, s(x|c), is computed using these probabilities with a given formula.
- This weighted score is then normalized again, this time ranging between -1 and 1 for final evaluation.
- The computational demand increases when size of AI labeler exceeds that of RM.

**Evaluation**

Results are evaluated with three metrics:

1. AI Labeler Alignment
    - This is a metric used to evaluate the accuracy of AI-labeled preferences in comparison with human preferences.
    - Step 1: The AI's soft-labeled preference (e.g., [0.6, 0.4]) is converted into a binary representation (e.g., [1, 0]).
    - Step 2: A score of '1' is assigned if the AI's label matches the human preference and '0' if it does not.
2. Win Rate
    - "Win Rate" is a metric used to evaluate the effectiveness of two different AI policies.
    - Given an input and two generated outputs (from policy A and B), human evaluators choose which output they prefer.
    - The "win rate" is calculated as the percentage of times policy A's output is preferred over policy B's.
3. Harmless Rate
    - "Harmless Rate" measures the percentage of responses that human evaluators consider harmless.
    - This metric is preferred over the "Win Rate" for evaluating harmless dialogue generation tasks, as many responses can be equally safe, making it challenging to assign relative rankings based on safety.

# Experimental Details