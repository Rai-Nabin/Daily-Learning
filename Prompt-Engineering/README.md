# Automated Prompt Generation
| S.N | Prompt Engineering Techniques | Overview | Year of Release | Note |
| :--: | ---- | ---- | :--: | :--: |
| 1 | Automatic Prompt Engineer (**APE**) | Automatic Prompt Engineering (APE) automates the creation and selection of optimal prompts for large language models (LLMs) by treating instructions as programs and optimizing them through a search over candidate prompts generated by an LLM. APE improves LLM performance across various tasks, often surpassing both previous baselines and human-crafted prompts, and enhances few-shot learning, zero-shot chain-of-thought prompting, and model output quality. | 2023 |  |
| 2 | Large Language Model as Optimizers (**OPRO**) | OPRO is a method that leverages large language models (LLMs) as optimizers for tasks without gradients. It works by using natural language prompts to iteratively generate and refine solutions. In each step, the LLM creates new solutions based on previously evaluated ones, which are then assessed and incorporated into subsequent prompts for further optimization. OPRO shows significant improvements in tasks like prompt optimization, outperforming human-designed prompts by notable margins. | 2024 |  |
| 3 | Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL (**Prompt-OIRL**) | Prompt-OIRL is a method designed to enhance the arithmetic reasoning abilities of Large Language Models (LLMs) by optimizing prompts through zero-shot learning. It addresses challenges like evaluating prompts without golden answers and the high resource cost of exploring prompt space. Prompt-OIRL uses offline inverse reinforcement learning (IRL) to learn a reward model from existing prompt demonstration data, allowing for effective query-dependent prompt evaluation. The best-of-N strategy is then applied to select the optimal prompt, improving both performance and efficiency in LLM tasks. | 2024 |  |
| 4 | **AutoPrompt**: Eliciting Knowledge from Language Models with Automatically Generated Prompts | AutoPrompt is an automated method for creating prompts that leverage masked language models (MLMs) to perform various tasks like sentiment analysis and natural language inference without additional parameters or fine-tuning. By using a gradient-guided search, AutoPrompt generates prompts that elicit accurate factual knowledge and can achieve performance comparable to state-of-the-art supervised models. It also demonstrates that MLMs can be effective in tasks like relation extraction, making AutoPrompt a viable, parameter-free alternative to manual prompt creation and traditional fine-tuning methods. | 2020 |  |
| 5 | **Prefix-Tuning**: Optimizing Continuous Prompts for Generations | Prefix-tuning is a lightweight alternative to fine-tuning large pretrained language models for natural language generation tasks. Instead of modifying all model parameters, prefix-tuning optimizes a small task-specific vector, called the prefix, while keeping the language model parameters frozen. This prefix acts like "virtual tokens" that subsequent tokens attend to during generation. Applied to models like GPT-2 and BART, prefix-tuning achieves comparable performance to full fine-tuning while only adjusting 0.1% of the parameters. It is particularly effective in low-data settings and better at generalizing to new topics. | 2021 |  |
| 6 | The Power of Scale for Parameter-Efficient Prompt Tuning (**Prompt Tuning**) | Prompt tuning is an effective method for adapting frozen language models to specific downstream tasks by learning "soft prompts" through backpropagation. Unlike discrete prompts, soft prompts are tuned with labeled examples and can significantly outperform GPT-3’s few-shot learning. As model sizes increase, prompt tuning becomes increasingly competitive with model tuning, matching its performance while only modifying a small portion of the model. This approach simplifies tasks such as domain transfer and prompt ensembling, and is particularly beneficial for reusing large models efficiently. Prompt tuning is also related to and can be seen as a simplification of prefix tuning. | 2021 |  |
| 7 | **ReAct** Prompting | ReAct is a method that integrates reasoning and action generation in large language models (LLMs) to enhance both language understanding and interactive decision-making. By combining reasoning traces with task-specific actions, ReAct improves model performance and interpretability. It enables the model to create, update, and track action plans, while also interfacing with external sources for additional information. Applied to question answering and fact verification, ReAct reduces issues like hallucination and error propagation by interacting with APIs. In interactive decision-making tasks, ReAct significantly outperforms traditional methods with fewer in-context examples, showing improvements in success rates and model interpretability. | 2023 |  |
| 8 | **Multimodal Chain-of-Thought** (CoT) Prompting | Multimodal-CoT integrates text and image modalities into chain-of-thought (CoT) prompting by using a two-stage framework. It first generates rationales based on both text and images, and then uses these multimodal rationales to enhance answer inference. This approach achieves state-of-the-art performance on benchmarks like ScienceQA and improves reasoning accuracy by mitigating hallucination and speeding up convergence. | 2023 |  |
