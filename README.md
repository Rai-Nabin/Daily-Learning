# Daily Learning Repository

Welcome to my daily learning repository! Here, I document new topics and concepts that I learn every day.
## Purpose
The main purpose of this repository is to track my learning progress and create a reference for myself and others interested in the topics I explore.

Feel free to explore and learn along with me!

~~*Latest LLM Research:* [note](../Recent-Research/README.md)~~
## Table of Contents

***3Blue1Brown Lecture videos**: [note](./3Blue1Brown/README.md)* 


*August 2024*

| S.N | Title | Note | Completion |
| :--: | ---- | ---- | ---- |
| 1 | Prompt techniques for Graph RAG |  |  |
| 2 | Federated Learning |  |  |
| 3 | The Future of Knowledge Assistants: Jerry Liu | [note](RAG/The-Future-of-Knowledge-Assistants.md) |  |
| 4 |  |  |  |

*July 2024*

| S.N | Title | Note | Completion |
| :--: | ---- | :--: | :--: |
| 1 | LLM Agents |  | &#x2610; |
| 3 | Automatic Prompt Engineering | [note](./Prompt-Engineering/README.md) |  |
| 4 | Prompt Evaluation Technique |  |  |

*June 2024*

| S.N | Title | Note | Completion |
| :--: | ---- | :--: | :--: |
| 1 | Explainable AI | [note](./Explainable-AI/README.md) |  |
| 2 | Security issues related to LLM |  |  |

*May 2024*

| S.N | Title | Note | Completion |
| :--: | ---- | :--: | :--: |
| 1 | Methods and Tools for Efficient Training on Single GPU | [note](./HuggingFace/Efficient-Training/README.md) | &#x2610; |
| 2 | LoRA and Its Derivatives |  | &#x2610; |
| 3 | Optimizing LLMs for Speed and Memory |  | &#x2610; |
| 4 | DAPT | [note](./DAPT/README.md) | &#x2611; |
| 5 | RAFT | note | &#x2610; |
| 6 | Techniques to Extend Context Length  |  |  |
| 7 | Falcon Series |  |  |

*April 2024*

| S.N | Title | Note | Completion |
| :--: | ---- | :--: | :--: |
| 1 | What is RAG? | [note](RAG/README.md) | &#x2610; |
| 2 | RAG vs Finetuning - Which is the Best Tool to Boost Your LLM Application? | [note](./RAG-vs-Finetuning/README.md) | &#x2610; |
| 3 | ZeRO & DeepSpeed | [note](DeepSpeed/README.md) | &#x2611; |
| 4 | Graph RAG | [note](./Graph-RAG/README.md) | &#x2610; |
| 5 | Model Merge Methods | [note](./Model-Merge/README.md) | &#x2610; |
| 6 | HuggingFace's model Memory Calculator | note | &#x2610; |
| 7 | RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback | [note](./RLAIF/README.md) | &#x2611; |
| 8 | Switch Transformers | [note](Daily-Learning/Switch-Transformers/README.md) | &#x2610; |
| 9 | Retrieval Meets Long Context LLMs | [note](./Retrieval/README.md) | &#x2610; |
| 10 | Training Language Models to Follow Instructions with Human Feedback | [note](./RLHF/README.md) | &#x2610; |
| 11 | Visual intro to transformers |  | &#x2610; |
| 12 | Visualizing Attention, a Transformer's Heart |  |  |
| 13 | Quantization in LLM |  |  |
| 14 | Distributed and Accelerated Model Training/Inference Libraries |  |  |
| 15 | 1-bit LLM |  |  |
| 16 | AI Constellations |  |  |
## Article/Blogs/Paper To Read
| S.N | Title | Resources |
| :--: | ---- | :--: |
| 1 | Let's Build GPT | [link](https://www.youtube.com/watch?v=kCc8FmEb1nY&ab_channel=AndrejKarpathy) |
| 2 | Let's Build the GPT Tokenizer | [link](https://www.youtube.com/watch?v=zduSFxRajkE&ab_channel=AndrejKarpathy) |
| 3 | Efficient Training on Multiple GPUs | [link](https://huggingface.co/docs/transformers/main/en/perf_train_gpu_many) |
| 4 | Mistral AI Business Model | [link](https://research.contrary.com/reports/mistral-ai) |
| 5 | Understanding and Using SFT for Language Models | [link](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised) |
| 6 | Understanding and Estimating GPU Memory Demands For Training LLMs in Practice | [link](https://medium.com/@maxshapp/understanding-and-estimating-gpu-memory-demands-for-training-llms-in-practise-c5ef20a4baff) |
| 7 | A Gentle Introduction to 8-bit Matrix Multiplication for Transformers at Scale | [link](https://huggingface.co/blog/hf-bitsandbytes-integration) |
| 8 | Can LLMs Learn from a Single Example? | [link](https://www.fast.ai/posts/2023-09-04-learning-jumps/) |
| 9 | Is DPO Always the Better Choice for Preference Tuning LLMs? | [link](https://deci.ai/blog/dpo-preference-tuning-llms/) |
| 10 | Efficient Large Language Models: A Survey | [link](https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey?tab=readme-ov-file) |
| 11 | Lets reproduce GPT-2 (124M) | [link](https://www.youtube.com/watch?v=l8pRSuU81PU&ab_channel=AndrejKarpathy) |
| 12 | Natural Language Processing at UT Austin | [link](https://www.youtube.com/playlist?list=PLofp2YXfp7TZZ5c7HEChs0_wfEfewLDs7) |
| 13 | Efficient ML  | [link](https://www.youtube.com/playlist?list=PL80kAHvQbh-pT4lCkDT53zT8DKmhE0idB) |
