**RAG**: [What is Retrieval-Augmented Generation?](https://research.ibm.com/blog/retrieval-augmented-generation-RAG?utm_medium=OSocial&utm_source=Youtube&utm_content=WAIWW&utm_id=YT-101-What-is-RAG&_gl=1*p6ef17*_ga*MTQwMzQ5NjMwMi4xNjkxNDE2MDc0*_ga_FYECCCS21D*MTY5MjcyMjgyNy40My4xLjE2OTI3MjMyMTcuMC4wLjA.)

Large language models usually give great answers, but because they're limited to the training data used to create the model, over time they can become incomplete--or worse, generate answers that are just plain wrong. 
One way of improving the LLM results is called "retrieval-augmented generation" or RAG.

RAG is an AI framework for retrieving facts from an external knowledge base to ground large language models (LLMs) on the most accurate, up-to-date information and to give users insight into LLMs' generative process.

Large language models (LLMs) demonstrate inconsistency in their responses, oscillating between accurate answers and the reproduction of random facts from their training data. This inconsistency is attributed to LLMs understanding how words relate statistically but lacking a profound understanding of their actual meanings, leading to moments where they may sound uncertain or incomprehensible.

Retrieval-augmented generation (RAG) is an AI framework that improves the quality of responses generated by large language models (LLMs) by grounding them in external sources of knowledge. Implemented in LLM-based question answering systems, RAG ensures access to current and reliable facts and provides transparency to users by allowing them to check the model's claims against its sources. Moreover, RAG reduces the risk of LLMs incorporating sensitive or incorrect information by relying on external, verifiable facts.

Retrieval-augmented generation (RAG) not only enhances response quality in large language models (LLMs) but also reduces the need for continuous training and parameter updates. This feature helps lower computational and financial costs, making RAG a cost-effective solution for LLM-powered chatbots in enterprise settings. RAG's ability to maintain relevance without constant retraining adds efficiency to dynamic environments.

---
**An ‘open book’ approach to answering tough questions**

The foundation of models like Large Language Models (LLMs) lies in the transformer, an AI architecture that transforms raw data into a compressed structure. While fine-tuning with domain-specific knowledge adapts models to various tasks, it falls short for specific questions in dynamic contexts. In 2020, Meta introduced retrieval-augmented generation (RAG), enabling LLMs to access information beyond training data. RAG operates like an open-book exam, allowing models to respond by browsing a specialized body of knowledge for more accurate answers, distinguishing it from closed-book models relying solely on memorized facts.

The retrieval-augmented generation (RAG) framework operates in two phases: retrieval and content generation. In the retrieval phase, algorithms seek and retrieve relevant information based on the user's query, drawing from a broad range of sources, depending on the domain (open or closed). This external knowledge is then combined with the user's prompt and presented to the language model. In the generative phase, the Large Language Model (LLM) uses the augmented prompt and its internal training data to produce a personalized and engaging response. The resulting answer, along with links to its sources, can be communicated to a chatbot for user interaction.

---

**Toward personalized and verifiable responses**

Before Large Language Models (LLMs), digital conversation agents relied on manual dialogue flows, where they confirmed customer intent, fetched information, and provided one-size-fits-all scripted answers. This manual decision-tree approach was effective for straightforward queries, but it had limitations. Scripting answers for every possible customer question was time-consuming, and if a scenario was missed, the chatbot couldn't improvise. Updating scripts as policies and circumstances evolved was often impractical or impossible, highlighting the need for more flexible and adaptive conversational AI systems.

LLM-powered chatbots now offer customers more personalized responses without requiring manual script writing. Retrieval-augmented generation (RAG) takes this a step further, significantly reducing the need for continuous model retraining. By simply uploading the latest documents or policies, the model, operating in open-book mode, can retrieve information to answer questions.

---
**Teaching the model to recognize when it doesn’t know**

Customer queries often pose challenges for Language Models (LLMs), as they can be ambiguously worded, complex, or require knowledge that the model may lack or struggle to comprehend. In these situations, LLMs may be prone to generating inaccurate responses or making things up. Unlike humans who learn to admit when they don't know something, LLMs need explicit training to recognize questions beyond their scope.

Fine-tuning a Language Model (LLM) can enable it to recognize when it's stuck and respond appropriately, but this process often requires exposure to thousands of examples of answerable and unanswerable questions. Only through such extensive training can the model effectively identify questions it can't answer and learn to ask for more details until it can provide a definitive response.

Currently, Retrieval-Augmented Generation (RAG) stands out as the most prominent tool for grounding LLMs in the latest, verifiable information, reducing the need for continuous retraining and updates. RAG relies on enriching prompts with relevant information stored in vectors, mathematical representations of data. These vectors, managed by efficient vector databases, facilitate the indexing, storage, and retrieval of information for applications like recommendation engines and chatbots. Despite its effectiveness, RAG is not without imperfections, and there are ongoing challenges in refining its implementation to achieve optimal results.


